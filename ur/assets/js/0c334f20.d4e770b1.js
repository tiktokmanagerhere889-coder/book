"use strict";(globalThis.webpackChunkwebsite=globalThis.webpackChunkwebsite||[]).push([[4522],{8453(e,n,i){i.d(n,{R:()=>s,x:()=>r});var t=i(6540);const o={},a=t.createContext(o);function s(e){const n=t.useContext(a);return t.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function r(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(o):e.components||o:s(e.components),t.createElement(a.Provider,{value:n},e.children)}},8552(e,n,i){i.r(n),i.d(n,{assets:()=>l,contentTitle:()=>r,default:()=>u,frontMatter:()=>s,metadata:()=>t,toc:()=>c});const t=JSON.parse('{"id":"modules/vla-integration/index","title":"Module 4: Vision-Language-Action (VLA)","description":"Teaching how natural language and vision are translated into physical robot actions","source":"@site/docs/modules/vla-integration/index.md","sourceDirName":"modules/vla-integration","slug":"/modules/vla-integration/","permalink":"/book/ur/docs/modules/vla-integration/","draft":false,"unlisted":false,"editUrl":"https://github.com/hassan/book/tree/master/docs/docs/modules/vla-integration/index.md","tags":[],"version":"current","sidebarPosition":9,"frontMatter":{"sidebar_position":9,"title":"Module 4: Vision-Language-Action (VLA)","description":"Teaching how natural language and vision are translated into physical robot actions"},"sidebar":"tutorialSidebar","previous":{"title":"Chapter 3: Navigation with Nav2","permalink":"/book/ur/docs/modules/isaac-ai-brain/chapter-3-navigation-with-nav2"},"next":{"title":"Chapter 1: Voice-to-Action","permalink":"/book/ur/docs/modules/vla-integration/chapter-1-voice-to-action"}}');var o=i(4848),a=i(8453);const s={sidebar_position:9,title:"Module 4: Vision-Language-Action (VLA)",description:"Teaching how natural language and vision are translated into physical robot actions"},r="Module 4: Vision-Language-Action (VLA)",l={},c=[{value:"Overview",id:"overview",level:2},{value:"Learning Objectives",id:"learning-objectives",level:2},{value:"Module Structure",id:"module-structure",level:2},{value:"Prerequisites",id:"prerequisites",level:2},{value:"Getting Started",id:"getting-started",level:2}];function d(e){const n={a:"a",h1:"h1",h2:"h2",header:"header",li:"li",ol:"ol",p:"p",ul:"ul",...(0,a.R)(),...e.components};return(0,o.jsxs)(o.Fragment,{children:[(0,o.jsx)(n.header,{children:(0,o.jsx)(n.h1,{id:"module-4-vision-language-action-vla",children:"Module 4: Vision-Language-Action (VLA)"})}),"\n",(0,o.jsx)(n.h2,{id:"overview",children:"Overview"}),"\n",(0,o.jsx)(n.p,{children:"Welcome to the Vision-Language-Action (VLA) module, where we explore how artificial intelligence systems translate human language and visual input into meaningful robot actions. This module is designed for AI and robotics engineers who want to understand how to integrate Large Language Models (LLMs) with robot control systems."}),"\n",(0,o.jsx)(n.h2,{id:"learning-objectives",children:"Learning Objectives"}),"\n",(0,o.jsx)(n.p,{children:"After completing this module, you will be able to:"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsx)(n.li,{children:"Understand the voice-to-action pipeline using OpenAI Whisper and similar technologies"}),"\n",(0,o.jsx)(n.li,{children:"Explain how cognitive planning with LLMs translates natural language into action sequences"}),"\n",(0,o.jsx)(n.li,{children:"Design end-to-end VLA pipelines for autonomous humanoid systems"}),"\n",(0,o.jsx)(n.li,{children:"Integrate navigation, perception, and manipulation workflows in the context of VLA"}),"\n"]}),"\n",(0,o.jsx)(n.h2,{id:"module-structure",children:"Module Structure"}),"\n",(0,o.jsx)(n.p,{children:"This module is organized into three comprehensive chapters:"}),"\n",(0,o.jsxs)(n.ol,{children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.a,{href:"/book/ur/docs/modules/vla-integration/chapter-1-voice-to-action",children:"Chapter 1: Voice-to-Action"})," - Learn about speech-to-text systems and mapping voice commands to robot intents"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.a,{href:"/book/ur/docs/modules/vla-integration/chapter-2-cognitive-planning",children:"Chapter 2: Cognitive Planning with LLMs"})," - Explore how natural language is translated into action sequences using LLMs"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.a,{href:"/book/ur/docs/modules/vla-integration/chapter-3-autonomous-humanoid",children:"Chapter 3: Capstone \u2013 The Autonomous Humanoid"})," - Implement an end-to-end VLA pipeline combining all concepts"]}),"\n"]}),"\n",(0,o.jsx)(n.h2,{id:"prerequisites",children:"Prerequisites"}),"\n",(0,o.jsx)(n.p,{children:"Before starting this module, you should have a solid understanding of:"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsx)(n.li,{children:"Basic ROS 2 concepts (covered in Module 1)"}),"\n",(0,o.jsx)(n.li,{children:"Digital twin simulation principles (covered in Module 2)"}),"\n",(0,o.jsx)(n.li,{children:"NVIDIA Isaac technologies (covered in Module 3)"}),"\n"]}),"\n",(0,o.jsx)(n.h2,{id:"getting-started",children:"Getting Started"}),"\n",(0,o.jsx)(n.p,{children:"Begin with Chapter 1 to understand the fundamentals of converting voice commands into robot actions, then progress through each chapter to build your knowledge of VLA systems incrementally."})]})}function u(e={}){const{wrapper:n}={...(0,a.R)(),...e.components};return n?(0,o.jsx)(n,{...e,children:(0,o.jsx)(d,{...e})}):d(e)}}}]);