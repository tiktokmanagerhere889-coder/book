"use strict";(globalThis.webpackChunkwebsite=globalThis.webpackChunkwebsite||[]).push([[4724],{8453(n,e,i){i.d(e,{R:()=>l,x:()=>o});var s=i(6540);const t={},r=s.createContext(t);function l(n){const e=s.useContext(r);return s.useMemo(function(){return"function"==typeof n?n(e):{...e,...n}},[e,n])}function o(n){let e;return e=n.disableParentContext?"function"==typeof n.components?n.components(t):n.components||t:l(n.components),s.createElement(r.Provider,{value:e},n.children)}},9559(n,e,i){i.r(e),i.d(e,{assets:()=>a,contentTitle:()=>o,default:()=>g,frontMatter:()=>l,metadata:()=>s,toc:()=>c});const s=JSON.parse('{"id":"modules/vla-integration/chapter-2-cognitive-planning","title":"Chapter 2: Cognitive Planning with LLMs","description":"Using Large Language Models for translating natural language to action sequences","source":"@site/docs/modules/vla-integration/chapter-2-cognitive-planning.md","sourceDirName":"modules/vla-integration","slug":"/modules/vla-integration/chapter-2-cognitive-planning","permalink":"/book/ur/docs/modules/vla-integration/chapter-2-cognitive-planning","draft":false,"unlisted":false,"editUrl":"https://github.com/hassan/book/tree/master/docs/docs/modules/vla-integration/chapter-2-cognitive-planning.md","tags":[],"version":"current","sidebarPosition":11,"frontMatter":{"sidebar_position":11,"title":"Chapter 2: Cognitive Planning with LLMs","description":"Using Large Language Models for translating natural language to action sequences"},"sidebar":"tutorialSidebar","previous":{"title":"Chapter 1: Voice-to-Action","permalink":"/book/ur/docs/modules/vla-integration/chapter-1-voice-to-action"},"next":{"title":"Chapter 3: Capstone \u2013 The Autonomous Humanoid","permalink":"/book/ur/docs/modules/vla-integration/chapter-3-autonomous-humanoid"}}');var t=i(4848),r=i(8453);const l={sidebar_position:11,title:"Chapter 2: Cognitive Planning with LLMs",description:"Using Large Language Models for translating natural language to action sequences"},o="Chapter 2: Cognitive Planning with LLMs",a={},c=[{value:"Introduction to Cognitive Planning",id:"introduction-to-cognitive-planning",level:2},{value:"Role of LLMs in Robotics",id:"role-of-llms-in-robotics",level:2},{value:"LLM Capabilities for Robotics",id:"llm-capabilities-for-robotics",level:3},{value:"Translating Natural Language to Action Sequences",id:"translating-natural-language-to-action-sequences",level:2},{value:"1. Language Understanding",id:"1-language-understanding",level:3},{value:"2. World Modeling",id:"2-world-modeling",level:3},{value:"3. Plan Generation",id:"3-plan-generation",level:3},{value:"4. Execution Monitoring",id:"4-execution-monitoring",level:3},{value:"ROS 2 Task Planning Concepts",id:"ros-2-task-planning-concepts",level:2},{value:"Action Servers and Clients",id:"action-servers-and-clients",level:3},{value:"Service Calls for Information",id:"service-calls-for-information",level:3},{value:"Topic Communication",id:"topic-communication",level:3},{value:"Cognitive Planning Architectures",id:"cognitive-planning-architectures",level:2},{value:"Hierarchical Planning",id:"hierarchical-planning",level:3},{value:"Reactive Planning",id:"reactive-planning",level:3},{value:"Collaborative Planning",id:"collaborative-planning",level:3},{value:"Practical Implementation Example",id:"practical-implementation-example",level:2},{value:"LLM Integration Patterns",id:"llm-integration-patterns",level:2},{value:"Prompt Engineering for Robotics",id:"prompt-engineering-for-robotics",level:3},{value:"Chain-of-Thought Reasoning",id:"chain-of-thought-reasoning",level:3},{value:"Few-Shot Learning",id:"few-shot-learning",level:3},{value:"Challenges and Solutions",id:"challenges-and-solutions",level:2},{value:"Ambiguity Resolution",id:"ambiguity-resolution",level:3},{value:"Execution Feasibility",id:"execution-feasibility",level:3},{value:"Temporal Reasoning",id:"temporal-reasoning",level:3},{value:"Best Practices",id:"best-practices",level:2},{value:"For LLM Integration",id:"for-llm-integration",level:3},{value:"For ROS 2 Integration",id:"for-ros-2-integration",level:3},{value:"Emerging Trends",id:"emerging-trends",level:2},{value:"Multimodal LLMs",id:"multimodal-llms",level:3},{value:"Continuous Learning",id:"continuous-learning",level:3},{value:"Summary",id:"summary",level:2},{value:"Hands-On Exercises",id:"hands-on-exercises",level:2},{value:"Exercise 1: LLM-ROS Integration",id:"exercise-1-llm-ros-integration",level:3},{value:"Exercise 2: Plan Validation System",id:"exercise-2-plan-validation-system",level:3},{value:"Exercise 3: Context-Aware Planning",id:"exercise-3-context-aware-planning",level:3},{value:"Learning Objectives Review",id:"learning-objectives-review",level:2},{value:"Next Steps",id:"next-steps",level:2}];function d(n){const e={a:"a",code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,r.R)(),...n.components};return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(e.header,{children:(0,t.jsx)(e.h1,{id:"chapter-2-cognitive-planning-with-llms",children:"Chapter 2: Cognitive Planning with LLMs"})}),"\n",(0,t.jsx)(e.h2,{id:"introduction-to-cognitive-planning",children:"Introduction to Cognitive Planning"}),"\n",(0,t.jsx)(e.p,{children:"Cognitive planning in robotics involves the translation of high-level natural language instructions into executable action sequences. This process requires sophisticated understanding of both linguistic meaning and the physical capabilities of the robot. Large Language Models (LLMs) have revolutionized this field by providing unprecedented capabilities for natural language understanding and reasoning."}),"\n",(0,t.jsx)(e.h2,{id:"role-of-llms-in-robotics",children:"Role of LLMs in Robotics"}),"\n",(0,t.jsx)(e.p,{children:"Large Language Models serve as the cognitive bridge between human intentions and robot actions. They excel at:"}),"\n",(0,t.jsxs)(e.ul,{children:["\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Natural Language Understanding"}),": Interpreting complex, nuanced human instructions"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Reasoning and Inference"}),": Drawing conclusions from implicit information"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Planning and Sequencing"}),": Breaking down complex tasks into executable steps"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Context Awareness"}),": Understanding situational and environmental factors"]}),"\n"]}),"\n",(0,t.jsx)(e.h3,{id:"llm-capabilities-for-robotics",children:"LLM Capabilities for Robotics"}),"\n",(0,t.jsx)(e.p,{children:"Modern LLMs bring several advantages to robotic systems:"}),"\n",(0,t.jsxs)(e.ul,{children:["\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Generalization"}),": Ability to handle novel situations not explicitly programmed"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Flexibility"}),": Interpretation of varied natural language expressions"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Knowledge Integration"}),": Leveraging vast world knowledge for decision-making"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Adaptability"}),": Adjusting behavior based on context and constraints"]}),"\n"]}),"\n",(0,t.jsx)(e.h2,{id:"translating-natural-language-to-action-sequences",children:"Translating Natural Language to Action Sequences"}),"\n",(0,t.jsx)(e.p,{children:"The process of converting natural language into robot actions involves several stages:"}),"\n",(0,t.jsx)(e.h3,{id:"1-language-understanding",children:"1. Language Understanding"}),"\n",(0,t.jsx)(e.p,{children:"Parsing and interpreting the user's instruction:"}),"\n",(0,t.jsxs)(e.ul,{children:["\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Semantic Analysis"}),": Extracting meaning from the natural language"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Entity Recognition"}),": Identifying objects, locations, and people"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Intent Classification"}),": Determining the overall goal or purpose"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Constraint Identification"}),": Recognizing limitations and requirements"]}),"\n"]}),"\n",(0,t.jsx)(e.h3,{id:"2-world-modeling",children:"2. World Modeling"}),"\n",(0,t.jsx)(e.p,{children:"Creating a representation of the current state and desired outcome:"}),"\n",(0,t.jsxs)(e.ul,{children:["\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Current State Assessment"}),": Understanding the robot's position and capabilities"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Goal State Definition"}),": Defining the desired end result"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Environment Context"}),": Incorporating environmental constraints and opportunities"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Resource Availability"}),": Assessing available tools and resources"]}),"\n"]}),"\n",(0,t.jsx)(e.h3,{id:"3-plan-generation",children:"3. Plan Generation"}),"\n",(0,t.jsx)(e.p,{children:"Creating a sequence of actions to achieve the goal:"}),"\n",(0,t.jsxs)(e.ul,{children:["\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"High-Level Planning"}),": Decomposing the task into major steps"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Low-Level Sequencing"}),": Converting steps into specific robot commands"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Resource Allocation"}),": Assigning appropriate robot capabilities to each step"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Timing Considerations"}),": Scheduling actions with temporal constraints"]}),"\n"]}),"\n",(0,t.jsx)(e.h3,{id:"4-execution-monitoring",children:"4. Execution Monitoring"}),"\n",(0,t.jsx)(e.p,{children:"Overseeing the plan execution and adapting as needed:"}),"\n",(0,t.jsxs)(e.ul,{children:["\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Progress Tracking"}),": Monitoring the execution of each step"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Error Detection"}),": Identifying deviations from expected outcomes"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Plan Adjustment"}),": Modifying the plan based on new information"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Feedback Integration"}),": Incorporating sensory feedback into planning"]}),"\n"]}),"\n",(0,t.jsx)(e.h2,{id:"ros-2-task-planning-concepts",children:"ROS 2 Task Planning Concepts"}),"\n",(0,t.jsx)(e.p,{children:"Integrating LLM-based cognitive planning with ROS 2 requires understanding several key concepts:"}),"\n",(0,t.jsx)(e.h3,{id:"action-servers-and-clients",children:"Action Servers and Clients"}),"\n",(0,t.jsx)(e.p,{children:"LLM-based planning can interface with ROS 2 action servers to execute complex tasks:"}),"\n",(0,t.jsxs)(e.ul,{children:["\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Navigation Actions"}),": Using nav2_msgs for movement and path planning"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Manipulation Actions"}),": Utilizing moveit_msgs for arm and gripper control"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Perception Actions"}),": Leveraging vision_msgs for object detection and recognition"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Custom Actions"}),": Creating domain-specific action interfaces"]}),"\n"]}),"\n",(0,t.jsx)(e.h3,{id:"service-calls-for-information",children:"Service Calls for Information"}),"\n",(0,t.jsx)(e.p,{children:"LLMs can trigger ROS 2 services to gather information:"}),"\n",(0,t.jsxs)(e.ul,{children:["\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"State Queries"}),": Retrieving current robot state and sensor data"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Map Requests"}),": Accessing environment maps and navigation capabilities"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Object Information"}),": Querying object databases and recognition systems"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Capability Checks"}),": Verifying robot capabilities before planning"]}),"\n"]}),"\n",(0,t.jsx)(e.h3,{id:"topic-communication",children:"Topic Communication"}),"\n",(0,t.jsx)(e.p,{children:"Continuous information flow between LLM planner and robot systems:"}),"\n",(0,t.jsxs)(e.ul,{children:["\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Sensor Data"}),": Receiving real-time sensory information"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Robot State"}),": Monitoring current position, battery, and system status"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Environmental Updates"}),": Tracking dynamic changes in the environment"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Progress Reports"}),": Broadcasting execution status and intermediate results"]}),"\n"]}),"\n",(0,t.jsx)(e.h2,{id:"cognitive-planning-architectures",children:"Cognitive Planning Architectures"}),"\n",(0,t.jsx)(e.p,{children:"Several architectural patterns emerge when integrating LLMs with robotic systems:"}),"\n",(0,t.jsx)(e.h3,{id:"hierarchical-planning",children:"Hierarchical Planning"}),"\n",(0,t.jsx)(e.p,{children:"Organizing planning into multiple levels of abstraction:"}),"\n",(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{children:"High-Level (LLM) \u2192 Mid-Level (Behavior Trees) \u2192 Low-Level (ROS Actions)\n"})}),"\n",(0,t.jsxs)(e.ul,{children:["\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Strategic Level"}),": Long-term goals and mission planning"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Tactical Level"}),": Task decomposition and resource allocation"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Operational Level"}),": Specific action execution and control"]}),"\n"]}),"\n",(0,t.jsx)(e.h3,{id:"reactive-planning",children:"Reactive Planning"}),"\n",(0,t.jsx)(e.p,{children:"Combining pre-planned sequences with real-time adaptation:"}),"\n",(0,t.jsxs)(e.ul,{children:["\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Plan Templates"}),": Predefined patterns for common tasks"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Context Injection"}),": Real-time environmental information"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Dynamic Replanning"}),": Adjusting plans based on unexpected situations"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Fallback Mechanisms"}),": Alternative strategies when primary plans fail"]}),"\n"]}),"\n",(0,t.jsx)(e.h3,{id:"collaborative-planning",children:"Collaborative Planning"}),"\n",(0,t.jsx)(e.p,{children:"Integrating human feedback and oversight:"}),"\n",(0,t.jsxs)(e.ul,{children:["\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Plan Validation"}),": Human review of proposed action sequences"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Preference Integration"}),": Incorporating human preferences and priorities"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Safety Overrides"}),": Human intervention capabilities"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Learning from Corrections"}),": Improving future plans based on human feedback"]}),"\n"]}),"\n",(0,t.jsx)(e.h2,{id:"practical-implementation-example",children:"Practical Implementation Example"}),"\n",(0,t.jsx)(e.p,{children:'Consider the instruction: "Please bring me the blue water bottle from the conference room, but if it\'s empty, bring the green one instead."'}),"\n",(0,t.jsx)(e.p,{children:"The cognitive planning process would involve:"}),"\n",(0,t.jsxs)(e.ol,{children:["\n",(0,t.jsxs)(e.li,{children:["\n",(0,t.jsxs)(e.p,{children:[(0,t.jsx)(e.strong,{children:"Language Understanding"}),": Identify the primary goal (fetching a water bottle), conditional logic (blue vs. green), and location (conference room)"]}),"\n"]}),"\n",(0,t.jsxs)(e.li,{children:["\n",(0,t.jsxs)(e.p,{children:[(0,t.jsx)(e.strong,{children:"World Modeling"}),": Assess current robot state, location of conference room, inventory of water bottles"]}),"\n"]}),"\n",(0,t.jsxs)(e.li,{children:["\n",(0,t.jsxs)(e.p,{children:[(0,t.jsx)(e.strong,{children:"Plan Generation"}),":"]}),"\n",(0,t.jsxs)(e.ul,{children:["\n",(0,t.jsx)(e.li,{children:"Navigate to conference room"}),"\n",(0,t.jsx)(e.li,{children:"Locate water bottles"}),"\n",(0,t.jsx)(e.li,{children:"Inspect blue bottle for contents"}),"\n",(0,t.jsx)(e.li,{children:"If blue bottle has water: pick it up and return"}),"\n",(0,t.jsx)(e.li,{children:"Else: pick up green bottle and return"}),"\n"]}),"\n"]}),"\n",(0,t.jsxs)(e.li,{children:["\n",(0,t.jsxs)(e.p,{children:[(0,t.jsx)(e.strong,{children:"Execution Monitoring"}),": Track progress, handle any obstacles, and report completion"]}),"\n"]}),"\n"]}),"\n",(0,t.jsx)(e.h2,{id:"llm-integration-patterns",children:"LLM Integration Patterns"}),"\n",(0,t.jsx)(e.h3,{id:"prompt-engineering-for-robotics",children:"Prompt Engineering for Robotics"}),"\n",(0,t.jsx)(e.p,{children:"Designing effective prompts for robotic applications:"}),"\n",(0,t.jsxs)(e.ul,{children:["\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Role Definition"}),": Clearly defining the LLM's role as a planning assistant"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Context Provision"}),": Providing relevant environmental and capability information"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Format Specifications"}),": Ensuring output follows expected action sequence formats"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Constraint Enforcement"}),": Including safety and operational constraints"]}),"\n"]}),"\n",(0,t.jsx)(e.h3,{id:"chain-of-thought-reasoning",children:"Chain-of-Thought Reasoning"}),"\n",(0,t.jsx)(e.p,{children:"Enabling LLMs to explain their planning process:"}),"\n",(0,t.jsxs)(e.ul,{children:["\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Step-by-step Analysis"}),": Breaking down the reasoning process"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Alternative Consideration"}),": Exploring multiple possible approaches"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Justification"}),": Explaining why certain decisions were made"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Contingency Planning"}),": Considering alternative scenarios"]}),"\n"]}),"\n",(0,t.jsx)(e.h3,{id:"few-shot-learning",children:"Few-Shot Learning"}),"\n",(0,t.jsx)(e.p,{children:"Providing examples to guide the LLM's planning behavior:"}),"\n",(0,t.jsxs)(e.ul,{children:["\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Successful Plans"}),": Examples of well-executed action sequences"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Failure Cases"}),": Examples of problematic situations and corrections"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Edge Cases"}),": Examples of unusual or complex scenarios"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Domain-Specific Patterns"}),": Examples tailored to specific robotic environments"]}),"\n"]}),"\n",(0,t.jsx)(e.h2,{id:"challenges-and-solutions",children:"Challenges and Solutions"}),"\n",(0,t.jsx)(e.h3,{id:"ambiguity-resolution",children:"Ambiguity Resolution"}),"\n",(0,t.jsxs)(e.ul,{children:["\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Clarification Queries"}),": LLMs can generate questions to resolve ambiguous instructions"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Contextual Disambiguation"}),": Using environmental context to interpret ambiguous references"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Probabilistic Interpretation"}),": Ranking possible interpretations by likelihood"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"User Confirmation"}),": Seeking confirmation for critical ambiguities"]}),"\n"]}),"\n",(0,t.jsx)(e.h3,{id:"execution-feasibility",children:"Execution Feasibility"}),"\n",(0,t.jsxs)(e.ul,{children:["\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Capability Checking"}),": Verifying that planned actions match robot capabilities"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Constraint Validation"}),": Ensuring plans comply with environmental constraints"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Resource Verification"}),": Confirming availability of required resources"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Safety Assessment"}),": Evaluating potential risks in the proposed plan"]}),"\n"]}),"\n",(0,t.jsx)(e.h3,{id:"temporal-reasoning",children:"Temporal Reasoning"}),"\n",(0,t.jsxs)(e.ul,{children:["\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Duration Estimation"}),": Predicting time requirements for each action"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Synchronization"}),": Coordinating multiple simultaneous processes"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Deadline Management"}),": Adjusting plans based on timing constraints"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Parallel Execution"}),": Identifying opportunities for concurrent actions"]}),"\n"]}),"\n",(0,t.jsx)(e.h2,{id:"best-practices",children:"Best Practices"}),"\n",(0,t.jsx)(e.h3,{id:"for-llm-integration",children:"For LLM Integration"}),"\n",(0,t.jsxs)(e.ul,{children:["\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Structured Output Formats"}),": Define clear, consistent formats for action sequences"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Error Handling"}),": Design plans with built-in error detection and recovery"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Modularity"}),": Create reusable planning components for common tasks"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Verification"}),": Implement validation steps before executing LLM-generated plans"]}),"\n"]}),"\n",(0,t.jsx)(e.h3,{id:"for-ros-2-integration",children:"For ROS 2 Integration"}),"\n",(0,t.jsxs)(e.ul,{children:["\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Standard Message Types"}),": Use conventional ROS message types when possible"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Service Architecture"}),": Design clean interfaces between LLM planner and ROS nodes"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Monitoring Tools"}),": Implement comprehensive logging and monitoring"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Safety Protocols"}),": Establish clear safety checks and emergency procedures"]}),"\n"]}),"\n",(0,t.jsx)(e.h2,{id:"emerging-trends",children:"Emerging Trends"}),"\n",(0,t.jsx)(e.h3,{id:"multimodal-llms",children:"Multimodal LLMs"}),"\n",(0,t.jsx)(e.p,{children:"Integration of visual information with language understanding:"}),"\n",(0,t.jsxs)(e.ul,{children:["\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Visual Grounding"}),": Connecting language references to visual objects"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Scene Understanding"}),": Interpreting environmental context from images"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Action Visualization"}),": Generating visual previews of proposed actions"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Real-time Perception"}),": Incorporating live camera feeds into planning"]}),"\n"]}),"\n",(0,t.jsx)(e.h3,{id:"continuous-learning",children:"Continuous Learning"}),"\n",(0,t.jsx)(e.p,{children:"Enabling LLM-based planners to improve over time:"}),"\n",(0,t.jsxs)(e.ul,{children:["\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Experience Logging"}),": Recording planning successes and failures"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Feedback Integration"}),": Incorporating human feedback into future planning"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Performance Metrics"}),": Tracking planning effectiveness and efficiency"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Adaptive Refinement"}),": Adjusting planning strategies based on outcomes"]}),"\n"]}),"\n",(0,t.jsx)(e.h2,{id:"summary",children:"Summary"}),"\n",(0,t.jsx)(e.p,{children:"Cognitive planning with LLMs represents a paradigm shift in human-robot interaction, enabling robots to understand and execute complex natural language instructions. The integration of LLMs with ROS 2 systems requires careful consideration of planning architectures, communication patterns, and safety protocols. Success depends on proper prompt engineering, structured output formats, and robust validation mechanisms."}),"\n",(0,t.jsx)(e.h2,{id:"hands-on-exercises",children:"Hands-On Exercises"}),"\n",(0,t.jsx)(e.h3,{id:"exercise-1-llm-ros-integration",children:"Exercise 1: LLM-ROS Integration"}),"\n",(0,t.jsxs)(e.ol,{children:["\n",(0,t.jsx)(e.li,{children:"Set up an LLM interface that can generate ROS 2 action calls"}),"\n",(0,t.jsx)(e.li,{children:"Implement a simple planning loop that converts text instructions to robot actions"}),"\n",(0,t.jsx)(e.li,{children:"Test with basic navigation and manipulation commands"}),"\n",(0,t.jsx)(e.li,{children:"Evaluate the effectiveness of different prompt structures"}),"\n"]}),"\n",(0,t.jsx)(e.h3,{id:"exercise-2-plan-validation-system",children:"Exercise 2: Plan Validation System"}),"\n",(0,t.jsxs)(e.ol,{children:["\n",(0,t.jsx)(e.li,{children:"Design a validation system for LLM-generated action sequences"}),"\n",(0,t.jsx)(e.li,{children:"Implement safety checks and capability verification"}),"\n",(0,t.jsx)(e.li,{children:"Create a simulation environment for testing"}),"\n",(0,t.jsx)(e.li,{children:"Evaluate the system with various instruction complexities"}),"\n"]}),"\n",(0,t.jsx)(e.h3,{id:"exercise-3-context-aware-planning",children:"Exercise 3: Context-Aware Planning"}),"\n",(0,t.jsxs)(e.ol,{children:["\n",(0,t.jsx)(e.li,{children:"Integrate environmental sensors with the LLM planning process"}),"\n",(0,t.jsx)(e.li,{children:"Implement contextual adaptation based on real-time information"}),"\n",(0,t.jsx)(e.li,{children:"Test with dynamic environments and changing conditions"}),"\n",(0,t.jsx)(e.li,{children:"Measure the improvement in plan success rates"}),"\n"]}),"\n",(0,t.jsx)(e.h2,{id:"learning-objectives-review",children:"Learning Objectives Review"}),"\n",(0,t.jsx)(e.p,{children:"After completing this chapter, you should be able to:"}),"\n",(0,t.jsxs)(e.ul,{children:["\n",(0,t.jsx)(e.li,{children:"Explain how LLMs translate natural language into action sequences"}),"\n",(0,t.jsx)(e.li,{children:"Design cognitive planning architectures for robotic systems"}),"\n",(0,t.jsx)(e.li,{children:"Integrate LLM planning with ROS 2 task execution"}),"\n",(0,t.jsx)(e.li,{children:"Address challenges in ambiguity resolution and feasibility checking"}),"\n"]}),"\n",(0,t.jsx)(e.h2,{id:"next-steps",children:"Next Steps"}),"\n",(0,t.jsxs)(e.p,{children:["Continue to ",(0,t.jsx)(e.a,{href:"/book/ur/docs/modules/vla-integration/chapter-3-autonomous-humanoid",children:"Chapter 3: Capstone \u2013 The Autonomous Humanoid"})," to learn how to implement an end-to-end VLA pipeline combining all the concepts from previous chapters."]})]})}function g(n={}){const{wrapper:e}={...(0,r.R)(),...n.components};return e?(0,t.jsx)(e,{...n,children:(0,t.jsx)(d,{...n})}):d(n)}}}]);